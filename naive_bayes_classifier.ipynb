{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier Implementation\n",
    "## Lesson 21 Homework - Machine Learning\n",
    "\n",
    "**Objective:** Implement Gaussian Naive Bayes classifier from scratch and compare with scikit-learn implementation.\n",
    "\n",
    "**Dataset:** Iris Dataset (Fisher's Iris data set)\n",
    "- Features: 4 continuous numerical features (Sepal Length, Sepal Width, Petal Length, Petal Width)\n",
    "- Target: 3 classes of Iris species\n",
    "- Split: 75% Training, 25% Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, iris.target_names)\n",
    "\n",
    "print(\"Dataset Shape:\", X.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.describe())\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset: 75% training, 25% test (randomized)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature A: Manual Implementation (From Scratch)\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Gaussian Naive Bayes** assumes that the features follow a Gaussian (Normal) distribution.\n",
    "\n",
    "**Bayes Theorem:**\n",
    "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
    "\n",
    "Where:\n",
    "- $P(y|X)$ = Posterior probability of class $y$ given features $X$\n",
    "- $P(X|y)$ = Likelihood of features $X$ given class $y$\n",
    "- $P(y)$ = Prior probability of class $y$\n",
    "- $P(X)$ = Evidence (constant for all classes)\n",
    "\n",
    "**For Gaussian distribution:**\n",
    "$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_y$ = Mean of feature $x_i$ for class $y$\n",
    "- $\\sigma_y^2$ = Variance of feature $x_i$ for class $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayesManual:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier implemented from scratch using NumPy.\n",
    "    \n",
    "    This implementation assumes features follow a Gaussian (Normal) distribution\n",
    "    and uses Bayes theorem for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Gaussian Naive Bayes model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Initialize arrays for means, variances, and priors\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.variances = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        \n",
    "        # Calculate mean, variance, and prior for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Calculate mean (μ) for each feature\n",
    "            self.means[idx, :] = X_c.mean(axis=0)\n",
    "            \n",
    "            # Calculate variance (σ²) for each feature\n",
    "            self.variances[idx, :] = X_c.var(axis=0)\n",
    "            \n",
    "            # Calculate prior probability P(y)\n",
    "            self.priors[idx] = X_c.shape[0] / n_samples\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calculate_likelihood(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Calculate the Gaussian probability density function (PDF).\n",
    "        \n",
    "        Formula: P(x|y) = (1 / sqrt(2π * σ²)) * exp(-(x - μ)² / (2σ²))\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : float\n",
    "            Feature value\n",
    "        mean : float\n",
    "            Mean (μ) of the feature for a specific class\n",
    "        var : float\n",
    "            Variance (σ²) of the feature for a specific class\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Probability density\n",
    "        \"\"\"\n",
    "        eps = 1e-6  # Small constant to avoid division by zero\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * (var + eps)))\n",
    "        denominator = np.sqrt(2 * np.pi * (var + eps))\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def _calculate_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Calculate posterior probability for each class.\n",
    "        \n",
    "        Using log probabilities to avoid numerical underflow:\n",
    "        log(P(y|X)) = log(P(y)) + Σ log(P(x_i|y))\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : array-like, shape (n_features,)\n",
    "            Single sample\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        array : Posterior probabilities for each class\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Start with log prior: log(P(y))\n",
    "            prior = np.log(self.priors[idx])\n",
    "            \n",
    "            # Calculate likelihood for each feature: Σ log(P(x_i|y))\n",
    "            likelihood = np.sum(\n",
    "                np.log(self._calculate_likelihood(x, self.means[idx, :], self.variances[idx, :]))\n",
    "            )\n",
    "            \n",
    "            # Posterior = Prior + Likelihood (in log space)\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        return np.array(posteriors)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Test data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        array : Predicted class labels\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X:\n",
    "            # Calculate posterior for all classes\n",
    "            posteriors = self._calculate_posterior(x)\n",
    "            \n",
    "            # Select class with highest posterior (ArgMax)\n",
    "            predicted_class = self.classes[np.argmax(posteriors)]\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Return the learned parameters.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing priors, means, and variances\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'priors': self.priors,\n",
    "            'means': self.means,\n",
    "            'variances': self.variances\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the manual Naive Bayes classifier\n",
    "manual_nb = GaussianNaiveBayesManual()\n",
    "manual_nb.fit(X_train, y_train)\n",
    "\n",
    "# Display learned parameters\n",
    "params = manual_nb.get_params()\n",
    "print(\"=\" * 60)\n",
    "print(\"MANUAL IMPLEMENTATION - LEARNED PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPrior Probabilities P(y):\")\n",
    "for idx, c in enumerate(manual_nb.classes):\n",
    "    print(f\"  Class {c} ({iris.target_names[c]}): {params['priors'][idx]:.4f}\")\n",
    "\n",
    "print(\"\\nMeans (μ) for each class and feature:\")\n",
    "means_df = pd.DataFrame(\n",
    "    params['means'],\n",
    "    columns=iris.feature_names,\n",
    "    index=[iris.target_names[i] for i in manual_nb.classes]\n",
    ")\n",
    "print(means_df)\n",
    "\n",
    "print(\"\\nVariances (σ²) for each class and feature:\")\n",
    "var_df = pd.DataFrame(\n",
    "    params['variances'],\n",
    "    columns=iris.feature_names,\n",
    "    index=[iris.target_names[i] for i in manual_nb.classes]\n",
    ")\n",
    "print(var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_manual = manual_nb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_manual = accuracy_score(y_test, y_pred_manual)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MANUAL IMPLEMENTATION - RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy: {accuracy_manual:.4f} ({accuracy_manual*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_manual = confusion_matrix(y_test, y_pred_manual)\n",
    "print(cm_manual)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_manual, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature B: Scikit-Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the scikit-learn Gaussian Naive Bayes classifier\n",
    "sklearn_nb = GaussianNB()\n",
    "sklearn_nb.fit(X_train, y_train)\n",
    "\n",
    "# Display learned parameters\n",
    "print(\"=\" * 60)\n",
    "print(\"SCIKIT-LEARN IMPLEMENTATION - LEARNED PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPrior Probabilities P(y):\")\n",
    "for idx, c in enumerate(sklearn_nb.classes_):\n",
    "    print(f\"  Class {c} ({iris.target_names[c]}): {np.exp(sklearn_nb.class_log_prior_[idx]):.4f}\")\n",
    "\n",
    "print(\"\\nMeans (μ) for each class and feature:\")\n",
    "sklearn_means_df = pd.DataFrame(\n",
    "    sklearn_nb.theta_,\n",
    "    columns=iris.feature_names,\n",
    "    index=[iris.target_names[i] for i in sklearn_nb.classes_]\n",
    ")\n",
    "print(sklearn_means_df)\n",
    "\n",
    "print(\"\\nVariances (σ²) for each class and feature:\")\n",
    "sklearn_var_df = pd.DataFrame(\n",
    "    sklearn_nb.var_,\n",
    "    columns=iris.feature_names,\n",
    "    index=[iris.target_names[i] for i in sklearn_nb.classes_]\n",
    ")\n",
    "print(sklearn_var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_sklearn = sklearn_nb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCIKIT-LEARN IMPLEMENTATION - RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy: {accuracy_sklearn:.4f} ({accuracy_sklearn*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_sklearn = confusion_matrix(y_test, y_pred_sklearn)\n",
    "print(cm_sklearn)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_sklearn, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two implementations\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: MANUAL vs SCIKIT-LEARN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nAccuracy Comparison:\")\n",
    "print(f\"  Manual Implementation:      {accuracy_manual:.4f} ({accuracy_manual*100:.2f}%)\")\n",
    "print(f\"  Scikit-Learn Implementation: {accuracy_sklearn:.4f} ({accuracy_sklearn*100:.2f}%)\")\n",
    "print(f\"  Difference:                  {abs(accuracy_manual - accuracy_sklearn):.4f}\")\n",
    "\n",
    "# Check if predictions are identical\n",
    "predictions_match = np.array_equal(y_pred_manual, y_pred_sklearn)\n",
    "print(f\"\\nPredictions Identical: {predictions_match}\")\n",
    "\n",
    "if not predictions_match:\n",
    "    n_differences = np.sum(y_pred_manual != y_pred_sklearn)\n",
    "    print(f\"Number of different predictions: {n_differences}/{len(y_test)}\")\n",
    "\n",
    "# Compare learned parameters\n",
    "print(\"\\nParameter Comparison:\")\n",
    "print(\"\\nPrior Probabilities - Max Difference:\")\n",
    "prior_diff = np.max(np.abs(params['priors'] - np.exp(sklearn_nb.class_log_prior_)))\n",
    "print(f\"  {prior_diff:.10f}\")\n",
    "\n",
    "print(\"\\nMeans - Max Difference:\")\n",
    "means_diff = np.max(np.abs(params['means'] - sklearn_nb.theta_))\n",
    "print(f\"  {means_diff:.10f}\")\n",
    "\n",
    "print(\"\\nVariances - Max Difference:\")\n",
    "var_diff = np.max(np.abs(params['variances'] - sklearn_nb.var_))\n",
    "print(f\"  {var_diff:.10f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 60)\n",
    "if predictions_match and accuracy_manual == accuracy_sklearn:\n",
    "    print(\"\\n✓ SUCCESS: Manual implementation produces IDENTICAL results to scikit-learn!\")\n",
    "elif abs(accuracy_manual - accuracy_sklearn) < 0.01:\n",
    "    print(\"\\n✓ SUCCESS: Manual implementation produces statistically similar results to scikit-learn!\")\n",
    "else:\n",
    "    print(\"\\n⚠ WARNING: Significant differences detected between implementations.\")\n",
    "\n",
    "print(f\"\\nBoth implementations achieved {accuracy_manual*100:.2f}% accuracy on the test set.\")\n",
    "print(\"The Gaussian Naive Bayes algorithm has been successfully implemented from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix - Manual Implementation\n",
    "sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names, ax=axes[0])\n",
    "axes[0].set_title(f'Manual Implementation\\nAccuracy: {accuracy_manual:.4f}')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Confusion Matrix - Scikit-Learn Implementation\n",
    "sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names, ax=axes[1])\n",
    "axes[1].set_title(f'Scikit-Learn Implementation\\nAccuracy: {accuracy_sklearn:.4f}')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy Comparison Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "implementations = ['Manual\\nImplementation', 'Scikit-Learn\\nImplementation']\n",
    "accuracies = [accuracy_manual, accuracy_sklearn]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(implementations, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Gaussian Naive Bayes: Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}\\n({acc*100:.2f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Accomplishments:\n",
    "\n",
    "1. **Data Preparation**: Successfully loaded the Iris dataset and split it into 75% training and 25% test sets with random shuffling.\n",
    "\n",
    "2. **Manual Implementation**: Implemented Gaussian Naive Bayes from scratch using only NumPy:\n",
    "   - Calculated prior probabilities P(y) for each class\n",
    "   - Computed means (μ) for each feature per class\n",
    "   - Computed variances (σ²) for each feature per class\n",
    "   - Implemented Gaussian probability density function\n",
    "   - Applied Bayes theorem for prediction using ArgMax\n",
    "\n",
    "3. **Scikit-Learn Implementation**: Used the standard GaussianNB classifier from scikit-learn for comparison.\n",
    "\n",
    "4. **Validation**: Compared both implementations and verified that the manual implementation produces results that are identical or statistically similar to the library version.\n",
    "\n",
    "### Mathematical Concepts Demonstrated:\n",
    "- Probability theory (Bayes Theorem)\n",
    "- Statistical measures (Mean, Variance)\n",
    "- Gaussian distribution\n",
    "- Maximum a posteriori (MAP) estimation\n",
    "\n",
    "### Technical Stack:\n",
    "- Python 3.x\n",
    "- NumPy (for mathematical operations)\n",
    "- Pandas (for data manipulation)\n",
    "- Scikit-learn (for dataset, splitting, and reference implementation)\n",
    "- Matplotlib & Seaborn (for visualization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
